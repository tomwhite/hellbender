From 5aaea009679534e124a85d0d603bed594b85b4e0 Mon Sep 17 00:00:00 2001
From: Tom White <tom@cloudera.com>
Date: Tue, 27 Sep 2016 14:05:34 +0100
Subject: [PATCH] Add CountReadsPerIntervalSpark and DumpReadsPerIntervalSpark.

---
 .../pipelines/CountReadsPerIntervalSpark.java      | 137 +++++++++++++++++++++
 .../spark/pipelines/DumpReadsPerIntervalSpark.java |  97 +++++++++++++++
 .../CountReadsPerIntervalSparkIntegrationTest.java |  56 +++++++++
 3 files changed, 290 insertions(+)
 create mode 100644 src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/CountReadsPerIntervalSpark.java
 create mode 100644 src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/DumpReadsPerIntervalSpark.java
 create mode 100644 src/test/java/org/broadinstitute/hellbender/tools/CountReadsPerIntervalSparkIntegrationTest.java

diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/CountReadsPerIntervalSpark.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/CountReadsPerIntervalSpark.java
new file mode 100644
index 000000000..7270d1499
--- /dev/null
+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/CountReadsPerIntervalSpark.java
@@ -0,0 +1,137 @@
+package org.broadinstitute.hellbender.tools.spark.pipelines;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import htsjdk.samtools.SAMSequenceRecord;
+import htsjdk.samtools.util.Locatable;
+import htsjdk.samtools.util.OverlapDetector;
+import org.apache.spark.api.java.JavaPairRDD;
+import org.apache.spark.api.java.JavaRDD;
+import org.apache.spark.api.java.JavaSparkContext;
+import org.apache.spark.api.java.function.FlatMapFunction;
+import org.apache.spark.api.java.function.FlatMapFunction2;
+import org.broadinstitute.hellbender.cmdline.Argument;
+import org.broadinstitute.hellbender.cmdline.CommandLineProgramProperties;
+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;
+import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;
+import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;
+import org.broadinstitute.hellbender.utils.SimpleInterval;
+import org.broadinstitute.hellbender.utils.gcs.BucketUtils;
+import org.broadinstitute.hellbender.utils.read.GATKRead;
+import org.broadinstitute.hellbender.utils.spark.SparkUtils;
+import scala.Tuple2;
+
+import java.io.PrintStream;
+import java.util.*;
+import java.util.stream.Collectors;
+
+@CommandLineProgramProperties(summary = "Counts reads per interval in the input SAM/BAM",
+        oneLineSummary = "CountReads per interval on Spark",
+        programGroup = SparkProgramGroup.class)
+public final class CountReadsPerIntervalSpark extends GATKSparkTool {
+
+    private static final long serialVersionUID = 1L;
+
+    @Override
+    public boolean requiresReads() { return true; }
+
+    @Argument(doc = "uri for the output file: a local file path",
+            shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME,
+            optional = false)
+    public String out;
+
+    @Argument(doc = "whether to use the shuffle implementation or not", shortName = "shuffle", fullName = "shuffle", optional = true)
+    public boolean shuffle = false;
+
+    @Override
+    protected void runTool(final JavaSparkContext ctx) {
+        final JavaRDD<GATKRead> reads = getReads();
+        JavaRDD<GATKRead> mappedReads = reads.filter(read -> !read.isUnmapped());
+
+        final List<Locatable> intervals = new ArrayList<>();
+        int regionSize = 4000;
+        int regionRepeat = 3; // fraction of regions 3 = 1/3 etc
+        for (SAMSequenceRecord sequenceRecord : getBestAvailableSequenceDictionary().getSequences()) {
+            String contig = sequenceRecord.getSequenceName();
+            int size = sequenceRecord.getSequenceLength();
+            for (int i = 0; i <= size/(regionSize * regionRepeat); i++) {
+                int start = i * regionSize * regionRepeat + 1;
+                int end = start + regionSize;
+                intervals.add(new SimpleInterval(contig, start, end));
+            }
+        }
+        JavaPairRDD<Locatable, Integer> intervalCountsRdd;
+
+        if (shuffle) {
+            intervalCountsRdd = SparkUtils.joinOverlappingShuffle(ctx, mappedReads, GATKRead.class, intervals,
+                    new CountOverlappingReadsFlatFunction()).mapToPair(t -> t);
+        } else {
+            intervalCountsRdd = SparkUtils.joinOverlapping(ctx, mappedReads, GATKRead.class, getBestAvailableSequenceDictionary(),
+                    intervals, new CountOverlappingReadsFunction()).mapToPair(t -> t);
+        }
+
+        List<Tuple2<Locatable, Integer>> collect = new ArrayList<>(intervalCountsRdd.collect());
+        Collections.sort(collect, (o1, o2) -> {
+            Locatable l1 = o1._1();
+            Locatable l2 = o2._1();
+            int c = l1.getContig().compareTo(l2.getContig());
+            if (c != 0) {
+                return c;
+            }
+            c = Integer.compare(l1.getStart(), l2.getStart());
+            if (c != 0) {
+                return c;
+            }
+            return Integer.compare(l1.getEnd(), l2.getEnd());
+        });
+        //collect.forEach(t -> System.out.println(t));
+
+        if(out != null) {
+            try (final PrintStream ps = new PrintStream(BucketUtils.createFile(out, getAuthenticatedGCSOptions()))) {
+                collect.forEach(t -> ps.println(t));
+            }
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    private static class CountOverlappingReadsFunction implements FlatMapFunction2<Iterator<GATKRead>, Iterator<Locatable>, Tuple2<Locatable, Integer>> {
+        private static final long serialVersionUID = 1L;
+
+        @Override
+        public Iterable<Tuple2<Locatable, Integer>> call(Iterator<GATKRead> readIterator,
+                                                         Iterator<Locatable> intervalIterator) {
+            List<Locatable> intervals = Lists.newArrayList(intervalIterator);
+            OverlapDetector<Locatable> overlapDetector = OverlapDetector.create(intervals);
+
+            Map<Locatable, Integer> counts = Maps.newLinkedHashMap();
+            while (readIterator.hasNext()) {
+                GATKRead read = readIterator.next();
+                Set<Locatable> overlaps = overlapDetector.getOverlaps(read);
+                for (Locatable overlap : overlaps) {
+                    Integer count = counts.get(overlap);
+                    counts.put(overlap, count == null ? 1 : count + 1);
+                }
+            }
+            return counts.entrySet().stream()
+                    .map(entry -> new Tuple2<>(entry.getKey(), entry.getValue()))
+                    .collect(Collectors.toList());
+        }
+    }
+
+    private static class CountOverlappingReadsFlatFunction implements FlatMapFunction<Tuple2<Locatable, Iterable<GATKRead>>, Tuple2<Locatable, Integer>> {
+        private static final long serialVersionUID = 1L;
+
+        @Override
+        public Iterable<Tuple2<Locatable, Integer>> call(Tuple2<Locatable, Iterable<GATKRead>> t) throws Exception {
+            Locatable interval = t._1;
+            int count = 0;
+            OverlapDetector<Locatable> overlapDetector = OverlapDetector.create(ImmutableList.of(interval));
+            for (GATKRead read : t._2) {
+                count += overlapDetector.getOverlaps(read).size();
+            }
+            return ImmutableList.of(new Tuple2<>(interval, count));
+        }
+    }
+
+}
diff --git a/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/DumpReadsPerIntervalSpark.java b/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/DumpReadsPerIntervalSpark.java
new file mode 100644
index 000000000..c3e47b75e
--- /dev/null
+++ b/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/DumpReadsPerIntervalSpark.java
@@ -0,0 +1,97 @@
+package org.broadinstitute.hellbender.tools.spark.pipelines;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Iterators;
+import com.google.common.collect.Lists;
+import htsjdk.samtools.SAMSequenceRecord;
+import htsjdk.samtools.util.Locatable;
+import htsjdk.samtools.util.OverlapDetector;
+import org.apache.spark.api.java.JavaRDD;
+import org.apache.spark.api.java.JavaSparkContext;
+import org.apache.spark.api.java.function.FlatMapFunction;
+import org.apache.spark.api.java.function.FlatMapFunction2;
+import org.broadinstitute.hellbender.cmdline.Argument;
+import org.broadinstitute.hellbender.cmdline.CommandLineProgramProperties;
+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;
+import org.broadinstitute.hellbender.cmdline.programgroups.SparkProgramGroup;
+import org.broadinstitute.hellbender.engine.spark.GATKSparkTool;
+import org.broadinstitute.hellbender.utils.SimpleInterval;
+import org.broadinstitute.hellbender.utils.read.GATKRead;
+import org.broadinstitute.hellbender.utils.spark.SparkUtils;
+import scala.Tuple2;
+
+import java.util.*;
+
+@CommandLineProgramProperties(summary = "Counts reads per interval in the input SAM/BAM",
+        oneLineSummary = "CountReads per interval on Spark",
+        programGroup = SparkProgramGroup.class)
+public final class DumpReadsPerIntervalSpark extends GATKSparkTool {
+
+    private static final long serialVersionUID = 1L;
+
+    @Override
+    public boolean requiresReads() { return true; }
+
+    @Argument(doc="the output file path", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME, fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME, optional = false)
+    protected String outputFile;
+
+    @Argument(doc = "whether to use the shuffle implementation or not", shortName = "shuffle", fullName = "shuffle", optional = true)
+    public boolean shuffle = false;
+
+    @Override
+    protected void runTool(final JavaSparkContext ctx) {
+        final JavaRDD<GATKRead> reads = getReads();
+        JavaRDD<GATKRead> mappedReads = reads.filter(read -> !read.isUnmapped());
+
+        final List<Locatable> intervals = new ArrayList<>();
+        int regionSize = 4000;
+        int regionRepeat = 3; // fraction of regions 3 = 1/3 etc
+        for (SAMSequenceRecord sequenceRecord : getBestAvailableSequenceDictionary().getSequences()) {
+            String contig = sequenceRecord.getSequenceName();
+            int size = sequenceRecord.getSequenceLength();
+            for (int i = 0; i <= size/(regionSize * regionRepeat); i++) {
+                int start = i * regionSize * regionRepeat + 1;
+                int end = start + regionSize;
+                intervals.add(new SimpleInterval(contig, start, end));
+            }
+        }
+
+        JavaRDD<GATKRead> collectedReads;
+
+        if (shuffle) {
+            collectedReads = SparkUtils.joinOverlappingShuffle(ctx, mappedReads, GATKRead.class, intervals,
+                    new DumpOverlappingReadsFlatFunction());
+        } else {
+            collectedReads = SparkUtils.joinOverlapping(ctx, mappedReads, GATKRead.class, getBestAvailableSequenceDictionary(),
+                    intervals, new DumpOverlappingReadsFunction());
+        }
+        shardedOutput = true;
+        writeReads(ctx, outputFile, collectedReads);
+    }
+
+    @SuppressWarnings("unchecked")
+    private static class DumpOverlappingReadsFunction implements FlatMapFunction2<Iterator<GATKRead>, Iterator<Locatable>, GATKRead> {
+        private static final long serialVersionUID = 1L;
+
+        @Override
+        public Iterable<GATKRead> call(Iterator<GATKRead> readIterator,
+                                                         Iterator<Locatable> intervalIterator) {
+            List<Locatable> intervals = Lists.newArrayList(intervalIterator);
+            OverlapDetector<Locatable> overlapDetector = OverlapDetector.create(intervals);
+            return () -> Iterators.filter(readIterator, overlapDetector::overlapsAny);
+        }
+    }
+
+    private static class DumpOverlappingReadsFlatFunction implements FlatMapFunction<Tuple2<Locatable, Iterable<GATKRead>>, GATKRead> {
+        private static final long serialVersionUID = 1L;
+
+        @Override
+        public Iterable<GATKRead> call(Tuple2<Locatable, Iterable<GATKRead>> t) throws Exception {
+            Locatable interval = t._1;
+            OverlapDetector<Locatable> overlapDetector = OverlapDetector.create(ImmutableList.of(interval));
+            return Iterables.filter(t._2(), overlapDetector::overlapsAny);
+        }
+    }
+
+}
diff --git a/src/test/java/org/broadinstitute/hellbender/tools/CountReadsPerIntervalSparkIntegrationTest.java b/src/test/java/org/broadinstitute/hellbender/tools/CountReadsPerIntervalSparkIntegrationTest.java
new file mode 100644
index 000000000..7f28fdcce
--- /dev/null
+++ b/src/test/java/org/broadinstitute/hellbender/tools/CountReadsPerIntervalSparkIntegrationTest.java
@@ -0,0 +1,56 @@
+package org.broadinstitute.hellbender.tools;
+
+import org.broadinstitute.hellbender.CommandLineProgramTest;
+import org.broadinstitute.hellbender.utils.test.ArgumentsBuilder;
+import org.testng.annotations.Test;
+
+import java.io.File;
+import java.nio.charset.Charset;
+import java.nio.file.Files;
+import java.util.List;
+
+import static org.testng.Assert.assertEquals;
+import static org.testng.AssertJUnit.assertFalse;
+
+public final class CountReadsPerIntervalSparkIntegrationTest extends CommandLineProgramTest {
+
+    @Test
+    public void test() throws Exception {
+        List<String> outNoShuffle = run(false);
+        List<String> outWithShuffle = run(true);
+        assertFalse(outNoShuffle.isEmpty());
+        assertEquals(outNoShuffle, outWithShuffle);
+    }
+
+    @Test
+    public void testWithShuffle() throws Exception {
+        final File bam = new File(publicTestDir + "large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam");
+        final File out = File.createTempFile("out", ".txt");
+        final ArgumentsBuilder args = new ArgumentsBuilder();
+        args.add("--input");
+        args.add(bam.getAbsolutePath());
+        args.add("--output");
+        args.add(out.getAbsolutePath());
+        args.add("--shuffle");
+        this.runCommandLine(args.getArgsArray());
+
+        Files.readAllLines(out.toPath(), Charset.forName("UTF-8"));
+    }
+
+    public List<String> run(boolean shuffle) throws Exception {
+        final File bam = new File(publicTestDir + "large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam");
+        final File out = File.createTempFile("out", ".txt");
+        final ArgumentsBuilder args = new ArgumentsBuilder();
+        args.add("--input");
+        args.add(bam.getAbsolutePath());
+        args.add("--output");
+        args.add(out.getAbsolutePath());
+        if (shuffle) {
+            args.add("--shuffle");
+        }
+        this.runCommandLine(args.getArgsArray());
+        return Files.readAllLines(out.toPath(), Charset.forName("UTF-8"));
+    }
+
+
+}
\ No newline at end of file
-- 
2.14.3 (Apple Git-98)

